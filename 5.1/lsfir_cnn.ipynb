{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5fb2f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2945f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preproccessing images\n",
    "def read_images(path, num_images):\n",
    "    array = np.zeros([num_images, 64*32])\n",
    "    i = 0\n",
    "    for image in os.listdir(path):\n",
    "        img_path =  path +  \"\\\\\" + image\n",
    "        image =  Image.open(img_path, mode= \"r\")\n",
    "        data = np.asarray(image, dtype= \"uint8\")\n",
    "        data = data.flatten()\n",
    "        array[i,:] = data\n",
    "        i += 1\n",
    "    return array\n",
    "\n",
    "#read negative\n",
    "train_neg_path = r\"C:\\Users\\yagiz\\Desktop\\data\\LSIFIR\\Classification\\Train\\neg\"\n",
    "num_train_neg_images = 43390\n",
    "\n",
    "train_neg_array = read_images(train_neg_path, num_train_neg_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f6e4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_neg_tensor:  torch.Size([43390, 2048])\n",
      "torch.Size([43390])\n"
     ]
    }
   ],
   "source": [
    "#negative tensors\n",
    "x_train_neg_tensor = torch.from_numpy(train_neg_array)\n",
    "print(\"x_train_neg_tensor: \", x_train_neg_tensor.size())\n",
    "\n",
    "y_train_neg_tensor = torch.zeros(num_train_neg_images, dtype=torch.long)\n",
    "print(y_train_neg_tensor.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d5a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read positives\n",
    "train_pos_path = r\"C:\\Users\\yagiz\\Desktop\\data\\LSIFIR\\Classification\\Train\\pos\"\n",
    "num_train_pos_images = 10208\n",
    "train_pos_array = read_images(train_pos_path, num_train_pos_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dff1c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_pos_tensor:  torch.Size([10208, 2048])\n",
      "y_train_pos_tensor:  torch.Size([10208])\n"
     ]
    }
   ],
   "source": [
    "x_train_pos_tensor = torch.from_numpy(train_pos_array)\n",
    "print(\"x_train_pos_tensor: \", x_train_pos_tensor.size())\n",
    "\n",
    "y_train_pos_tensor = torch.ones(num_train_pos_images, dtype=torch.long)\n",
    "print(\"y_train_pos_tensor: \", y_train_pos_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61ec480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train:  torch.Size([53598, 2048])\n",
      "y train:  torch.Size([53598])\n"
     ]
    }
   ],
   "source": [
    "#concat train\n",
    "x_train = torch.cat((x_train_neg_tensor, x_train_pos_tensor), 0)\n",
    "y_train = torch.cat((y_train_neg_tensor, y_train_pos_tensor), 0)\n",
    "print(\"x train: \", x_train.size())\n",
    "print(\"y train: \", y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b479b542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test_negative_tensor:  torch.Size([20855, 2048])\n",
      "y_test_neg_tensor:  torch.Size([20855])\n",
      "x_test_pos_tensor:  torch.Size([5944, 2048])\n",
      "y_test_pos_tensor:  torch.Size([5944])\n",
      "x_test:  torch.Size([26799, 2048])\n",
      "y_test:  torch.Size([26799])\n"
     ]
    }
   ],
   "source": [
    "#test images\n",
    "#reading test negatives\n",
    "test_neg_path = r\"C:\\Users\\yagiz\\Desktop\\data\\LSIFIR\\Classification\\Test\\neg\"\n",
    "num_test_neg_images = 22050\n",
    "test_negative_array = read_images(test_neg_path, num_test_neg_images)\n",
    "x_test_neg_tensor = torch.from_numpy(test_negative_array[:20855,:])\n",
    "print(\"x_test_negative_tensor: \", x_test_neg_tensor.size())\n",
    "y_test_neg_tensor = torch.ones(20855, dtype= torch.long)\n",
    "print(\"y_test_neg_tensor: \", y_test_neg_tensor.size())\n",
    "\n",
    "#reading test positives\n",
    "test_pos_path = r\"C:\\Users\\yagiz\\Desktop\\data\\LSIFIR\\Classification\\Test\\pos\"\n",
    "num_test_pos_images = 5944\n",
    "test_pos_array = read_images(test_pos_path, num_test_pos_images)\n",
    "x_test_pos_tensor = torch.from_numpy(test_pos_array)\n",
    "print(\"x_test_pos_tensor: \", x_test_pos_tensor.size())\n",
    "y_test_pos_tensor = torch.ones(num_test_pos_images, dtype=torch.long)\n",
    "print(\"y_test_pos_tensor: \", y_test_pos_tensor.size())\n",
    "\n",
    "#concat test\n",
    "x_test = torch.cat((x_test_neg_tensor, x_test_pos_tensor), 0)\n",
    "y_test = torch.cat((y_test_neg_tensor, y_test_pos_tensor), 0)\n",
    "print(\"x_test: \", x_test.size())\n",
    "print(\"y_test: \", y_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47d0c70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d096d7cc10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing\n",
    "plt.imshow(x_train[45001,:].reshape(64, 32), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "232d4d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN IMPLEMENTING\n",
    "\n",
    "# -> Hyper Parameters\n",
    "num_epochs = 5000\n",
    "num_classes = 2\n",
    "batch_size = 6\n",
    "batch_size_t = 2\n",
    "learning_rate = 0.00001\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 16, 5)\n",
    "\n",
    "        self.fc1 = nn.Linear(16*13*5, 520) ## image is downscaled to 13 x 5 from 64 x 32. And we have 16 feature maps (last conv2d)\n",
    "        self.fc2 = nn.Linear(520, 130)\n",
    "        self.fc3 = nn.Linear(130, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = x.view(-1, 16*5*13)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d1721d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matching labels and data >>>>>>>>\n",
    "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "test = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size = batch_size_t, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "647e46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling net\n",
    "net = Net().to(device=device)\n",
    "\n",
    "#loss and optimizer\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d143fff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test:  0.0\n",
      "Accuracy train:  0.0\n",
      "Accuracy test:  0.0\n",
      "Accuracy train:  0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2776/3763348675.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## zero gradient --gradients are being summed, we want them to be zero for every epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## forward prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## calculating loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    215\u001b[0m                             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training the neural network\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "loss_list = []\n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(batch_size, 1, 64, 32) #reshaping\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        if use_gpu:\n",
    "            if torch.cuda.is_available():\n",
    "                inputs, labels  = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() ## zero gradient --gradients are being summed, we want them to be zero for every epoch\n",
    "        outputs = net(inputs) ## forward prediction\n",
    "        loss = criteria(outputs, labels) ## calculating loss\n",
    "        #back propogation >>> loss.backward\n",
    "        loss.backward()\n",
    "        optimizer.step() ## update weights\n",
    "\n",
    "    #test\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "\n",
    "            images = images.view(images.size(0), 1, 64, 32)\n",
    "            images = images.float()\n",
    "\n",
    "            if use_gpu:\n",
    "                if torch.cuda.is_available():\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            output = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            #correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    acc1 = (correct/total) * 100\n",
    "    print(\"Accuracy test: \", acc1)\n",
    "    test_accuracy.append(acc1)\n",
    "       \n",
    "    #train test\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            images, labels = data\n",
    "\n",
    "            images = images.view(images.size(0), 1, 64, 32)\n",
    "            images = images.float()\n",
    "\n",
    "            if use_gpu:\n",
    "                if torch.cuda.is_available():\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            output = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            #correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    acc2 = (correct/total) * 100\n",
    "    print(\"Accuracy train: \", acc2)\n",
    "    train_accuracy.append(acc2)\n",
    "\n",
    "print(\"Train is done.\")\n",
    "\n",
    "end_time = time.time()\n",
    "process_time = end_time - start_time / 60\n",
    "print(\"Process Time: \", process_time)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a41f622eef1e1121d138b235322f0564aa0f6257b03c3b9f8c89c84a20d8a00"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
